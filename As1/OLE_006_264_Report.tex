%	INF219
%	Oskar L. F. LerivÃ¥g
%	INF264 Exersice 2

\documentclass[a4paper, 12pt]{article}

%Bookmarks
\usepackage[colorlinks=true,urlcolor=cyan,linkcolor=black,citecolor=red,bookmarksopen=true]{hyperref}
\usepackage{bookmark}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{pgf,tikz}
\usepackage{mathrsfs}
\usepackage{listings}
\usetikzlibrary{arrows}
\usepackage{amssymb}
\usepackage{url}
\usepackage{epigraph}
\usepackage{pgfplots}

%Subfile
\usepackage{subfiles}
    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for grffile with XeLaTeX
    \def\Gread@@xetex#1{%
      \IfFileExists{"\Gin@base".bb}%
      {\Gread@eps{\Gin@base.bb}}%
      {\Gread@@xetex@aux#1}%
    }
    \makeatother

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}

%Images%
\usepackage{graphicx}
\usepackage{float}

%Margins
\usepackage{geometry}
\geometry{a4paper, margin=3cm}

%Citations
\usepackage[round]{natbib}
\bibliographystyle{plainnat}

\newcommand{\mysection}[1]{\section*{#1} \addcontentsline{toc}{section}{#1}}
\newcommand{\mysubsection}[1]{\subsection*{#1} \addcontentsline{toc}{subsection}{#1}}
\newcommand{\mysubsubsection}[1]{\subsubsection*{#1} \addcontentsline{toc}{subsubsection}{#1}}

\newcommand{\myFigure}[3]{\begin{figure}[h!]\centering\includegraphics[scale=#1]{figures/#2}\caption{#3}\end{figure}}

\newcommand{\mycitation}[1]{[\citet{#1}]}

\begin{document}

    % % % % % % % % % % % % % % % % %
    %
    %	FRONT PAGE
    %
    \input{./uib_frontpage.tex}

    % % % % % % % % % % % % % % % % %
    %
    %	TABLE OF CONTENTS
    %
    \pdfbookmark{\contentsname}{toc}
    \tableofcontents
    \newpage
    
    \mysection{Running the program}
    The program is written in Python 3, using the Jupyter notebook implementation. To execute the code you will need to have python3 installed, and from PIP:
    \begin{itemize}
    	\item JuPyter
    	\item Pandas
    	\item Numpy
    	\item sklearn
    \end{itemize}
    
    \mysection{Implementing the D3 algorithm}
    \mysubsection{About the task}
    Implementing the entire tree algorithm from scratch was a hard but immersive experience. Digging into the deepest part of development-solutions and creating a prediction algorithm was cool. Unfortunately most of the examples represented was from discrete decision-trees, and therefore the development towards that became the natural way. It did not become clear until much later that we should be able to handle continuous values as well.
    \\\\
    Due to this, I found a discrete example from the internet, and preceded to implement a solution which would solve that. I believe I was mostly successful in doing so. It is able to learn and predict, but trying to use the abalone dataset the inadequacy became obvious. Therefore i implemented a continuous solution in the genie calculations, but I was unable to implement this into the actual tree.
    
    \mysubsection{On the topic of language}
    Using python was a bad move, many many hours were wasted in simpler things like trying to loop trough the tables and extracting the correct values. This has resulted in a much lower result than expected. Still, using any other language might not have proven much better as this is the course-language and most machine-learning-apps apparently run python, giving much more internet resources to read.
    
    \mysubsection{The issue of continuous values}
    The solution to the continuous issue is simply to create a discrete "question" to determine from the value. This can be done by using an average (or better separators) of the numbers, creating a true false question of e.g. $x < 6.54$. Implementing this should not be too difficult but time is a limited resource.
    
    \mysubsection{reduced-error pruning}
    Pruning a decision-tree is rather important on larger datasets, as they do really tend to over fit. This simply means to replace leaf-nodes with the most used result, but my implementation does not store the probabilities which requires some re-work to implement this.
    
    \mysection{Evaluating}
    Evaluation becomes difficult as not everything is successfully implemented. Still we can test it's training accuracy (which better be 1), and then a test accuracy. On a smaller discrete dataset like the tennis dataset added, we can see that the performance is ok. Even though there is a lot of unnecessary calculations. The accuracy of that test is also decent, but could be much better. Doing any larger datasets shows a clear increase in complexity. We should also note that since there is no pruning, larger datasets should over-fit.
    \\\\
    When it comes to testing the different parameters, one would try looping over all of them, testing which one holds the better accuracy and performance. It looks as though sklearns $DecisionTreeClassifier$ uses Gini, but it it's faster than entropy I do not know (but i guess it depends on the dataset).
    \\\\
    Pruning we know that should most likely be done if the dataset is large, and pruning is not that costly since we were to use reduced-error pruning. But again it's hard to say anything without trying them all out.
    
    \mysection{Comparing}
    As the application was not completed I was not able to compare against the sklearns $DecisionTreeClassifier$. But I did implement it and make it predict. It does not yield any good results, but the performance is impressive. Neither does the $DecitionTreeClassifier$ support discrete values, so it's difficult to test on the tennis set as well.
	
	%\subfile{./Inf264-Assignment_4.tex}
		
	
    % % % % % % % % % % % % % % % % %
    %
    %	ADD REFERANCES
    %
    %\bibliography{citation-db}
    %\addcontentsline{toc}{section}{References}

\end{document}