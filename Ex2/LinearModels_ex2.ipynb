{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Univariate Regression\n",
    "In this part you have to first we try and implement the building blocks for a linear regression implementation. First of all, we need to make sure to have the necessary libraries installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install numpy and pandas first. If you have them installed already then pip won't install them\n",
    "# and you will get requirement already satisfied message. That's normal.\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load some points from the  \"unilinear.csv\" file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the X and y values from unilinear.csv\n",
    "datapoints = np.genfromtxt('unilinear.csv', delimiter=',')\n",
    "X = datapoints[:,0]\n",
    "X = X.reshape(-1,1)\n",
    "X.sort(axis=0)\n",
    "\n",
    "\n",
    "y = datapoints[:,1]\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "# let us plot the data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.scatter(X, y)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1\n",
    "Implement the closed form of linear regression which was taught in the lecture:\n",
    "##### Hints: \n",
    "1. Add a column of 1's to X using `np.hstack`. This column of 1's models for the y-intercept ($w_0$). \n",
    "2. Use `inv` function in numpy to find the inverse of a matrix. inv is defined in `numpy.linalg` module so will need to import it from it. \n",
    "3. To multiply two matrices `a` and `b`, use the following notation in python `a.dot(b)`\n",
    "4. To transpose a matrix `c` use `c.T`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Write your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Check the values of w_hat vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# plot the estimated line with datapoints showing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.2\n",
    "### Implementation of Gradient Descent\n",
    "Calculate the loss function of Mean Square Error (MSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(X, y, w0, w1):\n",
    "    # TODO\n",
    "    # calculate the sum of squared errors for the current W vector\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you need to evaluate the current function with respect to w0 and w1. In order to do that caculate the one step derivatives of the MSE with respect to w0 and w1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w0, w1):\n",
    "    # TODO\n",
    "    # calculate the gradient vector of partial derivatives of MSE with the formulat that you have found\n",
    "    \n",
    "    # make a gradient vector from the partial derivatives    \n",
    "    gradient_mse = np.array([w0_par_der, w1_par_der])\n",
    "    return gradient_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to update your $w_0$ and $w_1$ using a certain learning rate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(w0, w1, w0_par_der, w1_par_der, learning_rate):\n",
    "    \n",
    "    # make a vector of weights\n",
    "    weight_vector = np.array([w0, w1])\n",
    "    # make a gradient vector from the partial derivatives    \n",
    "    gradient_mse = np.array([w0_par_der, w1_par_der])\n",
    "    # TODO\n",
    "    # update the vector of W (w0,w1) using the update rule and return the value of updated weights\n",
    "    \n",
    "    # return the updated weight vector as a list\n",
    "    return np.ndarray.tolist(updated_weight_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to run multiple iterations of gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(X, y, starting_w0, starting_w1, learning_rate, num_iterations):\n",
    "    w0 = starting_w0\n",
    "    w1 = starting_w1\n",
    "    # TODO\n",
    "    # run the Gradient descent for num_iterations\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        print(f'Iteration {i+1}: w0={w0:0.5f}, w1={w1:0.5f}, mse={mse:0.5f}')\n",
    "    return [w0, w1, mse]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to test your implementation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.flatten()\n",
    "y = y.flatten()\n",
    "num_iterations = 40\n",
    "learning_rate = 0.1\n",
    "starting_w0 = 0 # initial y-intercept guess\n",
    "starting_w1 = 0 # initial slope guess\n",
    "\n",
    "[w0,w1,_] = run_optimization(X, y, starting_w0, starting_w1, learning_rate, num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it again with learning rate of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.flatten()\n",
    "y = y.flatten()\n",
    "num_iterations = 40\n",
    "learning_rate = 1\n",
    "starting_w0 = 0 # initial y-intercept guess\n",
    "starting_w1 = 0 # initial slope guess\n",
    "\n",
    "w0_prime,w1_prime,_ = run_optimization(X, y, starting_w0, starting_w1, learning_rate, num_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your results with the sklearn version of linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install sklearn if it isn't installed already \n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import LinearRegression class from sklearn.linear_model module\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# make a lin_reg object form the LinearRegression class\n",
    "lin_reg = LinearRegression()\n",
    "X = np.reshape(X,(100,1))\n",
    "y = np.reshape(y,(100,1))\n",
    "lin_reg.fit(X, y)\n",
    "print('y-intercept w1:', lin_reg.intercept_)\n",
    "print('slope w0:', lin_reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# TODO\n",
    "# plot the original data points as a scatter plot\n",
    "\n",
    "# plot the line that fits these points. \n",
    "y_ = \n",
    "y_GD = \n",
    "\n",
    "\n",
    "plt.plot(X, y_, color='r', label='sklearn fit')\n",
    "plt.plot(X, y_GD, color='b', label='GD fit')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.grid()\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Multivariate Regression\n",
    "\n",
    "In this part we will try to work with more complicated datasets. In order to import datasets into your models first we need to use pandas library for python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# make a dataframe of the data\n",
    "df = pd.read_csv('Psychology grades.csv')\n",
    "\n",
    "# show first five rows of df\n",
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to seperate features and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the last column and set it to the output or dependent varaible y\n",
    "y = df[['FINAL']]\n",
    "\n",
    "# Remove the first column and set the rest of the dataframe to X. This is the set of indepedent variables\n",
    "X = df.drop(columns=['FINAL'])\n",
    "\n",
    "# show first five rows of X\n",
    "X.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first five rows of y\n",
    "y.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1\n",
    "Now fit a Multivariate Linear Regression model to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hint: \n",
    "Use LinearRegression from sklearn in order to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# fit a linear regression using sklearn. LinearRegression to the exam dataset and show the values of intercept_ and coef_\n",
    "\n",
    "\n",
    "# Display the learned parameters\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find out the grade for 50, 60 and 70 in exams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# use the predict method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the datapoints from \"nonlinear.csv\" and \"nonlinear_val.csv\" file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "datapoints = np.genfromtxt('nonlinear.csv', delimiter=',')\n",
    "datapoints_val = np.genfromtxt('nonlinear_val.csv', delimiter=',')\n",
    "\n",
    "X = datapoints[:,0]\n",
    "X = X.reshape(-1,1)\n",
    "X.sort(axis=0)\n",
    "\n",
    "\n",
    "y = datapoints[:,1]\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "\n",
    "X_val = datapoints_val[:,0]\n",
    "X_val = X_val.reshape(-1,1)\n",
    "X_val.sort(axis =0)\n",
    "\n",
    "y_val = datapoints_val[:,1]\n",
    "y_val = y_val.reshape(-1,1)\n",
    "\n",
    "# plot it\n",
    "fig , (ax1,ax2) = plt.subplots(2,figsize=(8, 8), dpi=80)\n",
    "\n",
    "ax1.scatter(X,y)\n",
    "ax2.scatter(X_val,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2\n",
    "Fit a normal linear regression on training dataset and check the MSE on each training and validation sets and plot the resulting functions on two separate plots with each corresponding to each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "# TODO\n",
    "# Use sklearn to fit a linear regression on the training data\n",
    "\n",
    "y_ = \n",
    "y_val_ = \n",
    "# use mean_squared_error from sklearn.metrics to calculate MSE\n",
    "print(mean_squared_error(,))\n",
    "print(mean_squared_error(,))\n",
    "\n",
    "# plot the function \n",
    "fig , (ax1,ax2) = plt.subplots(2,figsize=(8, 8), dpi=80)\n",
    "ax1.scatter(X,y)\n",
    "ax2.scatter(X_val,y_val)\n",
    "\n",
    "ax1.plot(X,y_,color='r')\n",
    "ax2.plot(X_val,y_val_,color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define an RBF function as your basis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RBF(X,C,eps):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform datapoints according to the RBF functions with centers at (-5,-4,...,4,5) with epsilon of 0.1 to a 11-dimensional space (number of colums = 11):\n",
    "##### Hint: \n",
    "Use `np.hstack` in order to add new columns to the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# transform X and X_val to 12 dimensional space using RBF functions \n",
    "\n",
    "\n",
    "# check the dimensionf of transformed datapoints (it should be (100,11) and (20,11) )\n",
    "print(X_RBF.shape)\n",
    "print(X_RBF_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to fit a linear regression on the new X_RBF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Fit a linear regression model to the RBF-transformed data\n",
    "clf = LinearRegression()\n",
    "clf.fit(,)\n",
    "\n",
    "# find the predicted values\n",
    "y_= \n",
    "y_val_=\n",
    "\n",
    "#find the mean square error on the training and validation sets\n",
    "print(mean_squared_error(,))\n",
    "print(mean_squared_error(,))\n",
    "\n",
    "\n",
    "fig , (ax1,ax2) = plt.subplots(2,figsize=(8, 8), dpi=80)\n",
    "ax1.plot(X, y_, color='r', label='Fitted for RBF-transformed on training data')\n",
    "ax2.plot(X, y_, color='r', label='Fitted for RBF-transformed on training data')\n",
    "\n",
    "ax1.scatter(X,y)\n",
    "ax2.scatter(X_val,y_val)\n",
    "\n",
    "\n",
    "ax1.legend(loc='best')\n",
    "ax2.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat the same procedure but this time put your centers at (-5,-4.9,-4.8,...,4.8,4.9). (your new space should have 100 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# transform X to 100 dimensions\n",
    "\n",
    "# check the number of dimensions\n",
    "print (X_RBF_2.shape)\n",
    "print (X_RBF_val_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# repeat the last part on the new data\n",
    "\n",
    "\n",
    "\n",
    "y2_ = \n",
    "\n",
    "y2_val_=\n",
    "\n",
    "\n",
    "print(mean_squared_error(,))\n",
    "print(mean_squared_error(,))\n",
    "\n",
    "fig , (ax1,ax2) = plt.subplots(2,figsize=(8, 8), dpi=80)\n",
    "ax1.plot(X, y2_, color='r', label='Fitted for RBF-transformed on training data')\n",
    "ax2.plot(X, y2_, color='r', label='Fitted for RBF-transformed on training data')\n",
    "\n",
    "ax1.scatter(X,y)\n",
    "ax2.scatter(X_val,y_val)\n",
    "\n",
    "\n",
    "ax1.legend(loc='best')\n",
    "ax2.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solve overfitting use the Ridge regularization. plot the resulting fit and compare it with the last one.\n",
    "##### Hint: \n",
    "Use the `Ridge` from  `sklearn.linear_model` with `alpha=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Create Ridge regression object from Ridge class\n",
    "ridge_reg = \n",
    "\n",
    "yRidge_ = \n",
    "yRidge_val_ = \n",
    "\n",
    "# plot original data and predicted fit\n",
    "print(mean_squared_error(,))\n",
    "print(mean_squared_error(,))\n",
    "\n",
    "\n",
    "fig , (ax1,ax2) = plt.subplots(2,figsize=(8, 8), dpi=80)\n",
    "\n",
    "ax1.scatter(X, y, label='Original data')\n",
    "ax1.plot(X, yRidge_, color='b', label='Fit for Ridge_RBF-transformed data')\n",
    "\n",
    "ax2.scatter(X_val, y_val, label='Original data')\n",
    "ax2.plot(X, yRidge_, color='b', label='Fit for Ridge_RBF-transformed data')\n",
    "\n",
    "ax1.legend(loc='best')\n",
    "ax2.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# iris is a dictionary of key-value pairs. Each key-value pairs contains some information about the dataset.\n",
    "# Lets display a list of these keys and see what they hold\n",
    "list(iris.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `data`: holds the data of sepal and petal lengths and widths in four columns,\n",
    "- `target`: holds the class of each flower. These class are encoded as 0, 1, and 2,\n",
    "- `target_names`: holds the names of each of the flower classes,\n",
    "- `DESCR`: contains a detailed description of the dataset, and\n",
    "- `feature_names`: contains a list of name of the columns of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the petal width of the Iris-Versicolor and plot the data according to the labels of 1 and 0 (1 if Iris-Virginica, else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['DESCR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us get the petal width. It is present in the 4th column of data\n",
    "X = iris[\"data\"][:, 3] \n",
    "X = X.reshape(-1,1)\n",
    "X.sort(axis=0)\n",
    "\n",
    "# lets define a binaray variable that encodes whether a flower is Iris-Versicolor or not\n",
    "# Iris_virginca flower is encoded as a 2 in target  \n",
    "y = (iris[\"target\"] == 2).astype(np.int) # 1 if Iris-Virginica, else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.xlabel('Petal width (cm)')\n",
    "plt.ylabel('Iris-Virginica(1) \\n Not Iris-Virginica(0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a linear regression model to the Iris pedal width for Iris-Versicolor and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Fit a linear regression to the petal width\n",
    "y_ = \n",
    "\n",
    "plt.scatter(X, y, label='original data')\n",
    "plt.plot(X, y_, color='r', label='fit from linear regression')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit a logistic regression to the same feature and compare the results to the linear regression in a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = \n",
    "\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris-Versicolor Prob\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", label=\"not Iris-Versicolor Prob\")\n",
    "plt.scatter(X, y, label='data')\n",
    "plt.grid()\n",
    "\n",
    "plt.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the features of sepal width and length from the dataset and try to fit a logistic regression on these features and and classes. plot the Plot the decision boundaries \n",
    "##### Hint:\n",
    "1. Make an object from LogisticRegression class in sklearn as following: \n",
    "\n",
    "`multiclass_logreg_obj = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10)`.\n",
    "\n",
    "`multiclass_logreg_obj` is just a name. It could be any (appropriate) name you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# prepare the data\n",
    "X = \n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "# TODO \n",
    "# fit a multi class logistic regression to the dataset\n",
    "\n",
    "\n",
    "\n",
    "# Step size in the mesh:\n",
    "h = .02\n",
    "# Light colors for decision boundaries plots:\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "# Bold colors for training points scatterplots:\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "\n",
    "### Decision boundaries plotting:\n",
    "# Generate the axis associated to the first feature: \n",
    "x_min = \n",
    "x_max = \n",
    "\n",
    "x_axis = np.arange(x_min, x_max, h)\n",
    "\n",
    "# Generate the axis associated to the second feature:\n",
    "y_min = \n",
    "y_max = \n",
    "\n",
    "y_axis = np.arange(y_min, y_max, h)\n",
    "\n",
    "\n",
    "# Generate a meshgrid (2D grid) from the 2 axis:\n",
    "\n",
    "x_grid, y_grid = np.meshgrid(x_axis, y_axis)\n",
    "\n",
    "# Vectorize the grids into column vectors:\n",
    "x_grid_vectorized = x_grid.flatten()\n",
    "x_grid_vectorized = np.expand_dims(x_grid_vectorized, axis=1)\n",
    "y_grid_vectorized = y_grid.flatten()\n",
    "y_grid_vectorized = np.expand_dims(y_grid_vectorized, axis=1)\n",
    "\n",
    "# Concatenate the vectorized grids:\n",
    "concat_grids = np.concatenate((x_grid_vectorized, y_grid_vectorized),axis=1)\n",
    "\n",
    "\n",
    "# Predict concatenated features to get the decision boundaries:\n",
    "decision_boundaries = \n",
    "# Reshape the decision boundaries into a 2D matrix:\n",
    "decision_boundaries = \n",
    "# Plot the decision boundaries:\n",
    "plt.figure()\n",
    "plt.pcolormesh(, , , cmap=cmap_light)\n",
    "# Overlay the training points:\n",
    "plt.scatter(, , c=Y, cmap=cmap_bold, edgecolor='k', s=20)\n",
    "plt.xlim()\n",
    "plt.ylim()\n",
    "plt.xlabel()\n",
    "plt.ylabel()\n",
    "plt.title()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
