%	INF219
%	Oskar L. F. Leriv√•g
%	INF264 Exersice 1

\documentclass[a4paper, 12pt]{article}

%Bookmarks
\usepackage[colorlinks=true,urlcolor=cyan,linkcolor=black,citecolor=red,bookmarksopen=true]{hyperref}
\usepackage{bookmark}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{pgf,tikz}
\usepackage{mathrsfs}
\usepackage{listings}
\usetikzlibrary{arrows}
\usepackage{amssymb}
\usepackage{url}
\usepackage{epigraph}

%Images%
\usepackage{graphicx}
\usepackage{float}

%Margins
\usepackage{geometry}
\geometry{a4paper, margin=3cm}

%Citations
\usepackage[round]{natbib}
\bibliographystyle{plainnat}

\newcommand{\mysection}[1]{\section*{#1} \addcontentsline{toc}{section}{#1}}
\newcommand{\mysubsection}[1]{\subsection*{#1} \addcontentsline{toc}{subsection}{#1}}
\newcommand{\mysubsubsection}[1]{\subsubsection*{#1} \addcontentsline{toc}{subsubsection}{#1}}

\newcommand{\myFigure}[3]{\begin{figure}[h!]\centering\includegraphics[scale=#1]{figures/#2}\caption{#3}\end{figure}}

\newcommand{\mycitation}[1]{[\citet{#1}]}

\begin{document}

    % % % % % % % % % % % % % % % % %
    %
    %	FRONT PAGE
    %
    \input{./uib_frontpage.tex}

    % % % % % % % % % % % % % % % % %
    %
    %	TABLE OF CONTENTS
    %
    \pdfbookmark{\contentsname}{toc}
    \tableofcontents
    \newpage

    % % % % % % % % % % % % % % % % %
    %
    %	BEGIN
    %
	\mysection{Mathematical warm up}
	\mysubsection{Matrices}
	{
	\begin{enumerate}
	\item
	Following that $ad - bc \neq 0 $ we know that these matrices are invertible as both $1*2-1*-1\neq0$ and $2*-2-1*1\neq0$ holds true.
	\[
		A^{-1} = \begin{bmatrix}
		1 & -1 \\
		1 & 2
		\end{bmatrix}^{-1} = \frac{1}{3} \begin{bmatrix}
		2 & 1 \\
		-1 & 1
		\end{bmatrix} = \begin{bmatrix}
		\frac{2}{3} & \frac{1}{3} \\
		- \frac{1}{3} & \frac{1}{3}
		\end{bmatrix}
	\]
	\[
		B^{-1} = \begin{bmatrix}
		2 & 1 \\
		1 & -2
		\end{bmatrix}^{-1} = -\frac{1}{5} \begin{bmatrix}
		-2 & -1 \\
		-1 & 2
		\end{bmatrix}^{-1} = \begin{bmatrix}
		\frac{2}{5} & \frac{1}{5} \\
		\frac{1}{5} & -\frac{2}{5}
		\end{bmatrix}
	\]
	\item
	\[
		AB = \begin{bmatrix}
		(1*2)+(-1*1) & (1*1)+(-1*-2) \\
		(1*2)+(2*1) & (1*1)+(2*-2)
		\end{bmatrix} = \begin{bmatrix}
		1 & 3 \\
		4 & -3
		\end{bmatrix}
	\]\\
	Again, $AB$ is inversible due to $(1*-3)-(3*4)=-15 \neq 0$
	\[
		(AB)^{-1} = \begin{bmatrix}
		1 & 3 \\
		4 & -3
		\end{bmatrix}^{-1} = \frac{1}{15} \begin{bmatrix}
		3 & 3 \\
		4 & -1
		\end{bmatrix} = \begin{bmatrix}
		\frac{1}{5} & \frac{1}{5} \\
		\frac{4}{15} & -\frac{1}{15}
		\end{bmatrix}
	\]
	\[
		B^{-1}A^{-1} = \begin{bmatrix}
		\frac{2}{5} & \frac{1}{5} \\
		\frac{1}{5} & -\frac{2}{5}
		\end{bmatrix}\begin{bmatrix}
		\frac{2}{3} & \frac{1}{3} \\
		- \frac{1}{3} & \frac{1}{3}
		\end{bmatrix} = \begin{bmatrix}
		(\frac{2}{5} * \frac{2}{3}) + (\frac{1}{5} * -\frac{1}{3}) & (\frac{2}{5} * \frac{1}{3}) + (\frac{1}{5} * \frac{1}{3}) \\
		(\frac{1}{5} * \frac{2}{3}) + (-\frac{2}{5} * -\frac{1}{3}) & (\frac{1}{5} * \frac{1}{3}) + (-\frac{2}{5} * \frac{1}{3})
		\end{bmatrix} = \begin{bmatrix}
		\frac{1}{5} & \frac{1}{5} \\
		\frac{4}{15} & -\frac{1}{15}
		\end{bmatrix}
	\]
	\[
		(AB)^{-1} = B^{-1}A^{-1} = \begin{bmatrix}
		\frac{1}{5} & \frac{1}{5} \\
		\frac{4}{15} & -\frac{1}{15}
		\end{bmatrix}
	\]
	\item
	For transposing a simple $2 * 2$ matrix it is only to swap $b$ and $c$. Thus giving us the matrices
	\[
		A^T = \begin{bmatrix}
		1 & 1 \\
		-1 & 2
		\end{bmatrix} \quad B^T = \begin{bmatrix}
		2 & 1 \\
		1 & -2
		\end{bmatrix} \quad (AB)^T = \begin{bmatrix}
		1 & 4 \\
		3 & -3
		\end{bmatrix}
	\]
	\[
		B^TA^T = \begin{bmatrix}
		1 & 1 \\
		-1 & 2
		\end{bmatrix} \begin{bmatrix}
		2 & 1 \\
		1 & -2
		\end{bmatrix} = \begin{bmatrix}
		2+-1 & 2+2 \\
		1+2 & 1+-4
		\end{bmatrix} = \begin{bmatrix}
		1 & 4 \\
		3 & -3
		\end{bmatrix}
	\]
	\[
		B^TA^T = (AB)^T = \begin{bmatrix}
		1 & 4 \\
		3 & -3
		\end{bmatrix}
	\]
	\item
	We know the solution to the eigenvalues can be calculated by $det(\lambda I-D)=0$. Therefore given D we can see
	\[
		D = \begin{bmatrix}
		1 & 0 & 0 \\
		1 & 2 & 0 \\
		2 & 0 & 3
		\end{bmatrix} \quad \lambda I_3 = \begin{bmatrix}
		\lambda & 0 & 0 \\
		0 & \lambda & 0 \\
		0 & 0 & \lambda
		\end{bmatrix}
	\]
	\[
		\lambda I_3 - D = \begin{bmatrix}
		\lambda - 1 & 0 & 0 \\
		-1 & \lambda - 2 & 0 \\
		-2 & 0 & \lambda - 3
		\end{bmatrix}
	\]\\
	
	Now given the matrix the determinant does get simple at both $b$ and $c$ is zero, we're left with the equation $(\lambda - 1)(\lambda - 2)(\lambda - 3)=0$ giving the possible solutions of $(\lambda = 1 \vee \lambda = 2 \vee \lambda = 3)$
	
	\item
	Given the matrix $D$ and the lambdas from above, we should be able to find an associated eigenvector to each of the corresponding values.\\
	
	\begin{minipage}[t]{0.3\textwidth}
	$D - {\lambda}_1 I$
	\[
		\begin{bmatrix}
		0 & 0 & 0 \\
		1 & 1 & 0 \\
		2 & 0 & 2
		\end{bmatrix}
	\]
	\[
		\begin{bmatrix}
		1 & -1 & 2 \\
		1 & 1 & 0 \\
		0 & 0 & 0
		\end{bmatrix}
	\]
		\[
		\begin{bmatrix}
		1 & 1 & 0 \\
		0 & 1 & -1 \\
		0 & 0 & 0
		\end{bmatrix}
	\]
	\end{minipage} \quad \vline \quad \begin{minipage}[t]{0.3\textwidth}
	$D - {\lambda}_2 I$
	\[
		\begin{bmatrix}
		-1 & 0 & 0 \\
		1 & 0 & 0 \\
		2 & 0 & 1
		\end{bmatrix}
	\]
	\[
		\begin{bmatrix}
		1 & 0 & 1 \\
		0 & 0 & 1 \\
		0 & 0 & 0
		\end{bmatrix}
	\]
	\end{minipage} \quad \vline \quad \begin{minipage}[t]{0.3\textwidth}
	$D - {\lambda}_3 I$	
	\[
		\begin{bmatrix}
		-2 & 0 & 0 \\
		1 & -1 & 0 \\
		2 & 0 & 0
		\end{bmatrix}
	\]
	\[
		\begin{bmatrix}
		1 & 0 & 0 \\
		0 & 1 & 0 \\
		0 & 0 & 0
		\end{bmatrix}
	\]
	\end{minipage}
	
	\begin{minipage}[t]{0.3\textwidth}	
	\[
		E_{\lambda = 1} = \begin{bmatrix}
		-1 \\ 1 \\ 1
		\end{bmatrix}
	\]
	\end{minipage} \quad \vline \quad \begin{minipage}[t]{0.3\textwidth}	
	\[
		E_{\lambda = 2} = \begin{bmatrix}
		0 \\ 1 \\ 0
		\end{bmatrix}
	\]
	\end{minipage} \quad \vline \quad \begin{minipage}[t]{0.3\textwidth}	
	\[
		E_{\lambda = 3} = \begin{bmatrix}
		0 \\ 0 \\ 1
		\end{bmatrix}
	\]
	\end{minipage}
	
    \end{enumerate}
    }
    \newpage
    \mysubsection{Calculus}
    {
    \begin{enumerate}
    \item
    Finding the partial derivatives we simply remove the "unknown" variable and find the derivative.
    
    \[
    		{\partial}_x f(x,y) = 2x-4 = 2(x-2)
    \]
    \[
    		{\partial}_y f(x,y) = 2y+6 = 2(y+3)
    \]
    
    \item
    By using the partial derivatives from above, we can find their zero value and use that to find the critical point.
    
    \begin{minipage}[t]{0.5\textwidth}
	${\partial}_x f(x,y)$    
    \[
    		2x-4 = 0
    \]
    \[
    		2x = 4
    \]
    \[
    		x = 2
    \]
    \end{minipage} \quad \vline \quad \begin{minipage}[t]{0.5\textwidth}
	${\partial}_y f(x,y)$    
    \[
    		2y+6 = 0
    \]
    \[
    		2y = -6
    \]
    \[
    		y = -3
    \]    
    \end{minipage}

	Which means we find the critical point located at $(2,-3)$ where $f(2,-3) = 2^2 + -3^2 - (4*2) + (6*-3) + 13 = 0$ 
    
    \item
	Factoring both partial derivatives (and forgetting the one constant) we are left with the new function $f(x,y) = 2(x-2)^2 + 2(y+3)^2$\\
	
	We now have two linear functions with a null point, by that we can conclude that our function does hold a local minimum point there. Now since we have two functions of second degree they both only hold a single turning point which means that our function cannot hold more than one minimal/maximal point, and therefore the point has to be the global minimum.
	
    \end{enumerate}
    }
    \newpage
    \mysubsection{Probabilities}
    {
    \begin{enumerate}
    \item
    Choosing random students
    \begin{itemize}
    \item $P(x \in T_2) = 300/960 \approx 31\%$
    \item $P(x \in G_1) = 176/960 \approx 18.3\%$
    \item $P(x \notin T_3) = 300+320/960 \approx 65.6\%$
    \end{itemize}
    \item $P(A|B) = 176/320 \approx 55\%$
    \item $P(B|A) = 176/576 \approx 0.31\%$
    
    \item
    It is rather difficult to say anything about them? But the wording can be important, and they are not really connected. "What is the chance of a randomly chosen student being a girl if we only pull from the first grade" is one example.
    \end{enumerate}
	}    
    
    \newpage
    \mysection{Machine learning problems}
    \mysubsection{Rema1000}
    The task hereby required is to create a system which will assist in creating a dynamic warehouse based on the location of the store. e.g. a store close to known student housings could have a greater selection of fast foot and cheaper alternatives like "first price", while a suburban store could hold more baby products. If we expand even more on this idea we could also make a store "seasonal" by stocking up on ice-cream before 17. may, and alcohol before special events like new years eve. We could even predict which stores should stock up most on which Christmas foods, as some regions could prefer turkey over pork ribs. And then even assist in ordering by warning of products going "out of fashion", if a product is spoiling faster than being sold according to its price, or by warning about "unknown" seasons like the yearly Bonus-payout in the company across the street in February or cleaning supplies in November as that is when most students drop out.
    \\\\  
    Trying to simplify the stocking of the store, we will need to remove unnecessary items, and to do that we need to know which items are unnecessary. We could potentially count the amount of sales per item, and the date which they are sold. We could also count the amount of wastage with date for each product, and new stockings as fresh items arrive. If we can log the entire sale made as one we could also try grouping items together and place them efficiently as to guide customers on better routes trough the store to increase revenue.
    \\\\
    Deciding on which products to add to a NEW store is a slightly different problem as we cannot collect live data and adapt. Instead we will need to create the data from other stores. This in turn means that we could start collecting the data above from all other stores and give the stores labels depending on which people we think shop there. Even better would be to ask each customer to register and tell us themselves. Then we can enter the labels which we think will be relevant and get which products we can expect to sell the most of.
    \\\\
    Starting small we need to collect data on all the sales done in all stores, and to which customer the sale was made. Then we can measure the accuracy of the predictions by comparing predictions made on each transaction with the known customer labels. Once we have a steady result we can create a new system on the same dataset made to predict stock items based on our assumed labels in the new location.
    \\\\
    \begin{itemize}
    \item \textbf{T(Task):} Look at customers and transactions to assume which type of customer will buy which items. Then using the same dataset make reversed assumptions.
    \item \textbf{P(Performance):} Could be measured on the accuracy of all assumptions on customer labels made.
    \item \textbf{E(Experience):} The data needed to be collected here is as many transactions and the correct customer label as possible.
    \end{itemize}
    
    \mysubsection{Tesla}
    Creating automated/self-driving cars is a new world of technologies as the computer can only "simulate" the real world from its many sensors. To avoid accidents we need to first know what kinds of accidents we might want to try to stop, how to stop it, and then which sensors to use, and then understand the sensor data, then account for false positives.
    \\\\
    One of the easiest accidents to start with might be drivers falling asleep behind the wheel. We know this happens, and that we can often prevent it by simply warning the driver with sounds, and if no response happens then stop the car. We could also look at the distance to the car in front and depending on the speed and weather warn the driver or slow down.
    \\\\
    If the task simply becomes to tell if the driver is sleepy, and warn the driver/stop the car, we will need some data about the driver and possibly his driving. We do know that falling asleep happens gradually, and we can therefore assume that the driver will start responding later then expected. One sensor we can use is an eye-tracking sensor to detect if the eye-movement is normal, and measure the time between blinking and their length. Another sensor is could be placed with the front wheels to detect if the driver is driving too close to the lines over a longer period.
    \\\\
    To generate the test data here in a safe manner is important. For the eye-tracking one could possibly emulate the experience of both intoxicated and tired drivers in a simulator while using the same sensor to track their eye movement.
    \\\\
    \begin{itemize}
    \item \textbf{T(Task):} Read driver data and predict possible dangers before they cause an accident.
    \item \textbf{P(Performance):} Could be measured by testing with the predictions accuracy during simulation being scored by both the driver and a trainer.
    \item \textbf{E(Experience):} The data which will be needed is many hours of simulated driving with different people, their score and the trainers score.
    \end{itemize}
    
    
    \newpage
    
    \mysubsection{Industrial waste INC}
    In this case, could you just not move the workers even further away from the arms? Or maybe require the companies delivering said goods to label them with machine-readable labels like a bar-code or QR-code? Then replace the arms with just pushers?
    \\\\
    ANYWAY! Automating such a process will be very hard as they don't necessarily know what is coming in. All kinds of sensors could be used: cameras to estimate the size of the container, a weight to further estimate the volume, infra-red camera, Geiger counter, one would need to automate every test they otherwise would have done.
    \\\\
    \begin{itemize}
    \item \textbf{T(Task):} Rad all possible inputs on the container, then guessing which chemical it is, then activate sorting arms/pushers accordingly.
    \item \textbf{P(Performance):} Matching predictions against correct answer.
    \item \textbf{E(Experience):} There needs to be a great amount of sensors to properly sort all the different incoming chemicals. And the amount of learning date should also be large as this might be high risk.
    \end{itemize}
    
    \newpage
    \mysection{Iris dataset}
    
    
    % % % % % % % % % % % % % % % % %
    %
    %	ADD REFERANCES
    %
    %\bibliography{citation-db}
    %\addcontentsline{toc}{section}{References}

\end{document}